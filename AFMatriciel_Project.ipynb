{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AFMatriciel_Project.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Nk6M591C6y8g",
        "QFXT6MsO7Wrk",
        "_0mFO-G57ZLP",
        "dPFCaV067d0y",
        "94Hh0sex77yF"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "mX5jkgnd6qwn",
        "colab_type": "code",
        "outputId": "9640a58c-5c39-4a30-8366-803962a172e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUkYtE-n7kO2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = \"/content/drive/My Drive/M2/AFMatriciel\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nk6M591C6y8g",
        "colab_type": "text"
      },
      "source": [
        "### **Data Compilation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEF8agxz6rhh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import os\n",
        "from scipy.sparse import dok_matrix, csr_matrix\n",
        "from scipy import io\n",
        "import tarfile\n",
        "\n",
        "\n",
        "\n",
        "#################################################\n",
        "total_no_users = 2649429\n",
        "total_no_movies = 17770\n",
        "\n",
        "def process_content(content, D):\n",
        "    lines = content.split(\"\\n\")\n",
        "    id_movie = int(lines[0][:-1]) - 1\n",
        "    for i in range(1, len(lines)):\n",
        "        if lines[i] != '':\n",
        "            line = lines[i].split(\",\")\n",
        "            id_user = int(line[0]) - 1\n",
        "            rating = int(line[1])\n",
        "            D[id_user, id_movie] = rating\n",
        "    return D\n",
        "\n",
        "\n",
        "def rating_compiler(folder_name, out_path):\n",
        "    D = dok_matrix((total_no_users, total_no_movies))\n",
        "    res_listdir = os.listdir(folder_name)\n",
        "    number = len(res_listdir)\n",
        "    i = 0\n",
        "    for f in res_listdir:\n",
        "        if os.path.isfile(folder_name+f):\n",
        "            print(i, \" / \", number)\n",
        "            myfile = open(folder_name+f)\n",
        "            content = myfile.read()\n",
        "            myfile.close()\n",
        "            D = process_content(content, D)\n",
        "        i += 1\n",
        "    D = csr_matrix(D)             \n",
        "    io.savemat(out_path, {'X' : D})\n",
        "\n",
        "\n",
        "def rating_compiler2(tar_name, out_path):\n",
        "    D = dok_matrix((total_no_users, total_no_movies))\n",
        "    tar = tarfile.open(tar_name)\n",
        "    res_getmembers = tar.getmembers()\n",
        "    number = len(res_getmembers)\n",
        "    i = 0\n",
        "    for member in res_getmembers:\n",
        "        f = tar.extractfile(member)\n",
        "        if f is not None:    \n",
        "            print(i, \" / \", number)        \n",
        "            content = f.read()\n",
        "            f.close()\n",
        "            D = process_content(content.decode(), D)\n",
        "        i += 1\n",
        "    tar.close()\n",
        "    D = csr_matrix(D)             \n",
        "    io.savemat(out_path, {'X' : D})\n",
        "\n",
        "\n",
        "def extract_T_and_R(D_file_name, qualifying_file_name, out_T_path, out_R_path):\n",
        "    D = io.loadmat(D_file_name)['X']\n",
        "    myfile = open(qualifying_file_name)\n",
        "    content = myfile.read()\n",
        "    myfile.close()\n",
        "    lines = content.split(\"\\n\")\n",
        "    users, movies = set(), set()\n",
        "    for line in lines:\n",
        "        if line != '':\n",
        "            line_split = line.split(\",\")\n",
        "            if len(line_split) == 1:\n",
        "                # Movie id\n",
        "                movies.add(int(line_split[0][:-1]) - 1)\n",
        "            else:\n",
        "                # User id\n",
        "                users.add(int(line_split[0]) - 1)\n",
        "    T = D[list(users),:]\n",
        "    T = T[:,list(movies)]    \n",
        "    io.savemat(out_T_path, {'X' : T})\n",
        "    \n",
        "    movies2 = set(range(total_no_movies))\n",
        "    movies2 = movies2.difference(movies)\n",
        "    users2 = set(range(total_no_users))\n",
        "    users2 = users2.difference(users)\n",
        "    \n",
        "    R = D[list(users2),:]\n",
        "    R = R[:,list(movies2)]\n",
        "    io.savemat(out_R_path, {'X' : R})\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "#################################################\n",
        "if __name__ == \"__main__\":\n",
        "    rating_compiler2(path+\"/download/training_set.tar\", path+\"/D.mat\")\n",
        "    extract_T_and_R(path+\"/D.mat\", path+\"/download/qualifying.txt\", path+\"/T.mat\", path+\"/R.mat\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NN61ajtw7D0C",
        "colab_type": "text"
      },
      "source": [
        "### **1. Baseline Estimates**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_ws4k4XHlu6",
        "colab_type": "code",
        "outputId": "7fec3d7c-2a8b-4ef6-f930-ff2d0a10354e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "from scipy import io, sparse\n",
        "import numpy as np\n",
        "from itertools import groupby\n",
        "from operator import itemgetter\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "bu_index_file = path+\"/bu_index.data\"\n",
        "bi_index_file = path+\"/bi_index.data\"\n",
        "\n",
        "if not (os.path.isfile(bu_index_file) and os.path.isfile(bi_index_file)):\n",
        "    mat = io.loadmat(path+\"/T.mat\")['X']\n",
        "    \"\"\"mat = mat[1:5000,1:5000]\n",
        "    mat = mat[mat.getnnz(1)>0][:,mat.getnnz(0)>0]\"\"\"\n",
        "\n",
        "    # Pre-processing\n",
        "    print(\"Pre-processing...\")\n",
        "    mat_nonzero = mat.nonzero()\n",
        "\n",
        "    print(\"   make bi indexes...\")\n",
        "    bi_index = []\n",
        "    for k, g in groupby(zip(mat_nonzero[0], mat_nonzero[1]), itemgetter(0)):\n",
        "      to_add = list(map(lambda x:int(x[1]), list(g)))\n",
        "      bi_index.append(to_add)    \n",
        "\n",
        "    print(\"   make bu indexes...\")\n",
        "    bu_index = []\n",
        "    indexes = np.argsort(mat_nonzero[1])\n",
        "    for k, g in groupby(zip(mat_nonzero[1][indexes], mat_nonzero[0][indexes]), itemgetter(0)):\n",
        "      to_add = list(map(lambda x:int(x[1]), list(g)))\n",
        "      bu_index.append(to_add)    \n",
        "\n",
        "    with open(bi_index_file, \"wb\") as fp:\n",
        "        pickle.dump(bi_index, fp)\n",
        "    with open(bu_index_file, \"wb\") as fp:\n",
        "        pickle.dump(bu_index, fp)\n",
        "else:\n",
        "    with open(\"bi_index.data\", \"rb\") as fp:\n",
        "        bi_index = pickle.load(fp)\n",
        "    with open(\"bu_index.data\", \"rb\") as fp:\n",
        "        bu_index = pickle.load(fp)\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pre-processing...\n",
            "   make bi indexes...\n",
            "   make bu indexes...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5AUhGWy7qR1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "outputId": "4d5c4928-6186-4219-b73a-49b0cdaf6554"
      },
      "source": [
        "from scipy import io, sparse\n",
        "import numpy as np\n",
        "from itertools import groupby\n",
        "from operator import itemgetter\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "bu_index, bi_index = [], []\n",
        "\n",
        "def pre_processing():\n",
        "    bu_index_file = path+\"/bu_index.data\"\n",
        "    bi_index_file = path+\"/bi_index.data\"\n",
        "\n",
        "    if not (os.path.isfile(bu_index_file) and os.path.isfile(bi_index_file)):\n",
        "        mat = io.loadmat(path+\"/T.mat\")['X']\n",
        "        \"\"\"mat = mat[1:5000,1:5000]\n",
        "        mat = mat[mat.getnnz(1)>0][:,mat.getnnz(0)>0]\"\"\"\n",
        "\n",
        "        print(\"Pre-processing...\")\n",
        "        mat_nonzero = mat.nonzero()\n",
        "\n",
        "        print(\"   make bi indexes...\")\n",
        "        bi_index = []\n",
        "        for k, g in groupby(zip(mat_nonzero[0], mat_nonzero[1]), itemgetter(0)):\n",
        "          to_add = list(map(lambda x:int(x[1]), list(g)))\n",
        "          bi_index.append(to_add)    \n",
        "\n",
        "        print(\"   make bu indexes...\")\n",
        "        bu_index = []\n",
        "        indexes = np.argsort(mat_nonzero[1])\n",
        "        for k, g in groupby(zip(mat_nonzero[1][indexes], mat_nonzero[0][indexes]), itemgetter(0)):\n",
        "          to_add = list(map(lambda x:int(x[1]), list(g)))\n",
        "          bu_index.append(to_add)    \n",
        "\n",
        "        with open(bi_index_file, \"wb\") as fp:\n",
        "            pickle.dump(bi_index, fp)\n",
        "        with open(bu_index_file, \"wb\") as fp:\n",
        "            pickle.dump(bu_index, fp)\n",
        "    else:\n",
        "        with open(bi_index_file, \"rb\") as fp:\n",
        "            bi_index = pickle.load(fp)\n",
        "        with open(bu_index_file, \"rb\") as fp:\n",
        "            bu_index = pickle.load(fp)\n",
        "    return bu_index, bi_index\n",
        "\n",
        "\n",
        "def compute_loss(mat, mu, bu, bi, l_reg=0.02):\n",
        "  loss = 0\n",
        "\n",
        "  no_users_entries = np.array((mat != 0).sum(1)).T.ravel()\n",
        "  bu_rep = np.repeat(bu.ravel(), no_users_entries)\n",
        "\n",
        "  no_movies_entries = np.array((mat != 0).sum(0)).ravel()\n",
        "  bi_rep = np.repeat(bi.ravel(), no_movies_entries)\n",
        "\n",
        "  temp_mat = sparse.csc_matrix(mat).copy()\n",
        "  temp_mat.data[:] -= bi_rep\n",
        "  temp_mat.data[:] -= mu\n",
        "  temp_mat = sparse.coo_matrix(temp_mat)\n",
        "  temp_mat = sparse.csr_matrix(temp_mat)\n",
        "  temp_mat.data[:] -= bu_rep\n",
        "\n",
        "  loss = (temp_mat.data[:] ** 2).sum()\n",
        "\n",
        "  reg = l_reg * ((bu**2).sum() + (bi**2).sum())  \n",
        "  loss += reg\n",
        "\n",
        "  return loss\n",
        "\n",
        "\n",
        "def baseline_estimator(mat_file, l_reg=0.02, learning_rate=0.0000025):\n",
        "\n",
        "  mat = io.loadmat(mat_file)['X']\n",
        "  \"\"\"mat = mat[1:5000,1:5000]\n",
        "  mat = mat[mat.getnnz(1)>0][:,mat.getnnz(0)>0]\"\"\"\n",
        "\n",
        "  print(mat.shape)\n",
        "  no_users = mat.shape[0]\n",
        "  no_movies = mat.shape[1]\n",
        "  \n",
        "  bu = np.random.rand(no_users,1)  * 2 - 1\n",
        "  bi = np.random.rand(1,no_movies) * 2 - 1\n",
        "  #bu = np.zeros((no_users,1))\n",
        "  #bi = np.zeros((1,no_movies))  \n",
        "\n",
        "  mu = mat.data[:].mean()\n",
        "  mat_sum1 = mat.sum(1)\n",
        "  mat_sum0 = mat.sum(0)\n",
        "  n = mat.data[:].shape[0]\n",
        "\n",
        "  no_users_entries = np.array((mat != 0).sum(1))\n",
        "  no_movies_entries = np.array((mat != 0).sum(0))\n",
        "\n",
        "  # Train\n",
        "  print(\"Train...\")\n",
        "  n_iter = 200\n",
        "  for it in range(n_iter):\n",
        "\n",
        "    #bi_sum = bi[bi_index].sum(1).reshape((no_users,1))\n",
        "    #bu_sum = bu.ravel()[bu_index].sum(0).reshape((1,no_movies)) \n",
        "\n",
        "    bi_sum = np.array(list(map(lambda x:bi.ravel()[x].sum(), bi_index))).reshape((no_users,1))\n",
        "    bu_sum = np.array(list(map(lambda x:bu.ravel()[x].sum(), bu_index))).reshape((1,no_movies))    \n",
        "\n",
        "    bu_gradient = - 2.0 * (mat_sum1 - no_users_entries  * mu - no_users_entries  * bu - bi_sum) + 2.0 * l_reg * bu\n",
        "    bu -= learning_rate * bu_gradient \n",
        "\n",
        "    bi_gradient = - 2.0 * (mat_sum0 - no_movies_entries * mu - no_movies_entries * bi - bu_sum) + 2.0 * l_reg * bi\n",
        "    bi -= learning_rate * bi_gradient \n",
        " \n",
        "    \"\"\"print(bu.mean())    \n",
        "    print(bi.mean())    \n",
        "    print(bu_gradient.mean())\n",
        "    print(bi_gradient.mean())\"\"\"\n",
        "    if it % 10 == 0:\n",
        "      print(it, \"\\ \", n_iter)         \n",
        "      \"\"\"print(bu)\n",
        "      print(bi)      \n",
        "      print(bu_gradient)\n",
        "      print(bi_gradient)\"\"\"\n",
        "      print(\"compute loss...\")\n",
        "      print(compute_loss(mat, mu, bu, bi, l_reg=l_reg))\n",
        "\n",
        "  return bu, bi\n",
        "#################################################\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  \n",
        "  bu_index, bi_index = pre_processing()\n",
        "  baseline_estimator(path+\"/T.mat\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(478615, 17470)\n",
            "Train...\n",
            "0 \\  200\n",
            "compute loss...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-3530709b37c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m   \u001b[0mbu_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbi_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpre_processing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m   \u001b[0mbaseline_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/T.mat\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-24-3530709b37c7>\u001b[0m in \u001b[0;36mbaseline_estimator\u001b[0;34m(mat_file, l_reg, learning_rate)\u001b[0m\n\u001b[1;32m    121\u001b[0m       print(bi_gradient)\"\"\"\n\u001b[1;32m    122\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"compute loss...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_reg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ml_reg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mbu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-3530709b37c7>\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(mat, mu, bu, bi, l_reg)\u001b[0m\n\u001b[1;32m     59\u001b[0m   \u001b[0mtemp_mat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m   \u001b[0mtemp_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoo_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_mat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m   \u001b[0mtemp_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsr_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_mat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0mtemp_mat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mbu_rep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0marg1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                 \u001b[0marg1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_self\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36masformat\u001b[0;34m(self, format, copy)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0;31m# Forward the copy kwarg, if it's accepted.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/sparse/coo.py\u001b[0m in \u001b[0;36mtocsr\u001b[0;34m(self, copy)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             coo_tocsr(M, N, self.nnz, row, col, self.data,\n\u001b[0;32m--> 409\u001b[0;31m                       indptr, indices, data)\n\u001b[0m\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFXT6MsO7Wrk",
        "colab_type": "text"
      },
      "source": [
        "### **2. Correlation-Based Neighbourhood Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gGumOSo7rKa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0mFO-G57ZLP",
        "colab_type": "text"
      },
      "source": [
        "### **3. Correlation-Based Neighbourhood Model with Implicit Feedback**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7DjKnRT7rxM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPFCaV067d0y",
        "colab_type": "text"
      },
      "source": [
        "### **4. SVD++**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKG3HgK98Gym",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94Hh0sex77yF",
        "colab_type": "text"
      },
      "source": [
        "### **5. Integrated Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkJ8byiA8H0W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}