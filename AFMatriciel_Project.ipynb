{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AFMatriciel_Project.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Nk6M591C6y8g",
        "QFXT6MsO7Wrk",
        "_0mFO-G57ZLP",
        "dPFCaV067d0y",
        "94Hh0sex77yF"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "mX5jkgnd6qwn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUkYtE-n7kO2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = \"/content/drive/My Drive/M2/AFMatriciel\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nk6M591C6y8g",
        "colab_type": "text"
      },
      "source": [
        "### **Data Compilation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEF8agxz6rhh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import os\n",
        "from scipy.sparse import dok_matrix, csr_matrix\n",
        "from scipy import io\n",
        "import tarfile\n",
        "\n",
        "\n",
        "\n",
        "#################################################\n",
        "total_no_users = 2649429\n",
        "total_no_movies = 17770\n",
        "\n",
        "def process_content(content, D):\n",
        "    lines = content.split(\"\\n\")\n",
        "    id_movie = int(lines[0][:-1]) - 1\n",
        "    for i in range(1, len(lines)):\n",
        "        if lines[i] != '':\n",
        "            line = lines[i].split(\",\")\n",
        "            id_user = int(line[0]) - 1\n",
        "            rating = int(line[1])\n",
        "            D[id_user, id_movie] = rating\n",
        "    return D\n",
        "\n",
        "\n",
        "def rating_compiler(folder_name, out_path):\n",
        "    D = dok_matrix((total_no_users, total_no_movies))\n",
        "    res_listdir = os.listdir(folder_name)\n",
        "    number = len(res_listdir)\n",
        "    i = 0\n",
        "    for f in res_listdir:\n",
        "        if os.path.isfile(folder_name+f):\n",
        "            print(i, \" / \", number)\n",
        "            myfile = open(folder_name+f)\n",
        "            content = myfile.read()\n",
        "            myfile.close()\n",
        "            D = process_content(content, D)\n",
        "        i += 1\n",
        "    D = csr_matrix(D)             \n",
        "    io.savemat(out_path, {'X' : D})\n",
        "\n",
        "\n",
        "def rating_compiler2(tar_name, out_path):\n",
        "    D = dok_matrix((total_no_users, total_no_movies))\n",
        "    tar = tarfile.open(tar_name)\n",
        "    res_getmembers = tar.getmembers()\n",
        "    number = len(res_getmembers)\n",
        "    i = 0\n",
        "    for member in res_getmembers:\n",
        "        f = tar.extractfile(member)\n",
        "        if f is not None:    \n",
        "            print(i, \" / \", number)        \n",
        "            content = f.read()\n",
        "            f.close()\n",
        "            D = process_content(content.decode(), D)\n",
        "        i += 1\n",
        "    tar.close()\n",
        "    D = csr_matrix(D)             \n",
        "    io.savemat(out_path, {'X' : D})\n",
        "\n",
        "\n",
        "def extract_T_and_R(D_file_name, qualifying_file_name, out_T_path, out_R_path):\n",
        "    D = io.loadmat(D_file_name)['X']\n",
        "    myfile = open(qualifying_file_name)\n",
        "    content = myfile.read()\n",
        "    myfile.close()\n",
        "    lines = content.split(\"\\n\")\n",
        "    users, movies = set(), set()\n",
        "    for line in lines:\n",
        "        if line != '':\n",
        "            line_split = line.split(\",\")\n",
        "            if len(line_split) == 1:\n",
        "                # Movie id\n",
        "                movies.add(int(line_split[0][:-1]) - 1)\n",
        "            else:\n",
        "                # User id\n",
        "                users.add(int(line_split[0]) - 1)\n",
        "    T = D[list(users),:]\n",
        "    T = T[:,list(movies)]    \n",
        "    io.savemat(out_T_path, {'X' : T})\n",
        "    \n",
        "    movies2 = set(range(total_no_movies))\n",
        "    movies2 = movies2.difference(movies)\n",
        "    users2 = set(range(total_no_users))\n",
        "    users2 = users2.difference(users)\n",
        "    \n",
        "    R = D[list(users2),:]\n",
        "    R = R[:,list(movies2)]\n",
        "    io.savemat(out_R_path, {'X' : R})\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "#################################################\n",
        "if __name__ == \"__main__\":\n",
        "    rating_compiler2(path+\"/download/training_set.tar\", path+\"/D.mat\")\n",
        "    extract_T_and_R(path+\"/D.mat\", path+\"/download/qualifying.txt\", path+\"/T.mat\", path+\"/R.mat\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NN61ajtw7D0C",
        "colab_type": "text"
      },
      "source": [
        "### **1. Baseline Estimates**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5AUhGWy7qR1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy import io, sparse\n",
        "import numpy as np\n",
        "from itertools import groupby\n",
        "from operator import itemgetter\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "bu_index, bi_index = [], []\n",
        "\n",
        "def pre_processing():\n",
        "    bu_index_file = path+\"/bu_index.data\"\n",
        "    bi_index_file = path+\"/bi_index.data\"\n",
        "\n",
        "    if not (os.path.isfile(bu_index_file) and os.path.isfile(bi_index_file)):\n",
        "        mat = io.loadmat(path+\"/T.mat\")['X']\n",
        "        \"\"\"mat = mat[1:5000,1:5000]\n",
        "        mat = mat[mat.getnnz(1)>0][:,mat.getnnz(0)>0]\"\"\"\n",
        "\n",
        "        print(\"Pre-processing...\")\n",
        "        mat_nonzero = mat.nonzero()\n",
        "\n",
        "        print(\"   make bi indexes...\")\n",
        "        bi_index = []\n",
        "        for k, g in groupby(zip(mat_nonzero[0], mat_nonzero[1]), itemgetter(0)):\n",
        "          to_add = list(map(lambda x:int(x[1]), list(g)))\n",
        "          bi_index.append(to_add)    \n",
        "\n",
        "        print(\"   make bu indexes...\")\n",
        "        bu_index = []\n",
        "        indexes = np.argsort(mat_nonzero[1])\n",
        "        for k, g in groupby(zip(mat_nonzero[1][indexes], mat_nonzero[0][indexes]), itemgetter(0)):\n",
        "          to_add = list(map(lambda x:int(x[1]), list(g)))\n",
        "          bu_index.append(to_add)    \n",
        "\n",
        "        with open(bi_index_file, \"wb\") as fp:\n",
        "            pickle.dump(bi_index, fp)\n",
        "        with open(bu_index_file, \"wb\") as fp:\n",
        "            pickle.dump(bu_index, fp)\n",
        "    else:\n",
        "        with open(bi_index_file, \"rb\") as fp:\n",
        "            bi_index = pickle.load(fp)\n",
        "        with open(bu_index_file, \"rb\") as fp:\n",
        "            bu_index = pickle.load(fp)\n",
        "    return bu_index, bi_index\n",
        "\n",
        "\n",
        "def compute_loss(mat, mu, bu, bi, l_reg=0.02):\n",
        "  loss = 0\n",
        "\n",
        "  no_users_entries = np.array((mat != 0).sum(1)).T.ravel()\n",
        "  bu_rep = np.repeat(bu.ravel(), no_users_entries)\n",
        "\n",
        "  no_movies_entries = np.array((mat != 0).sum(0)).ravel()\n",
        "  bi_rep = np.repeat(bi.ravel(), no_movies_entries)\n",
        "\n",
        "  temp_mat = sparse.csc_matrix(mat).copy()\n",
        "  temp_mat.data[:] -= bi_rep\n",
        "  temp_mat.data[:] -= mu\n",
        "  temp_mat = sparse.coo_matrix(temp_mat)\n",
        "  temp_mat = sparse.csr_matrix(temp_mat)\n",
        "  temp_mat.data[:] -= bu_rep\n",
        "\n",
        "  loss = (temp_mat.data[:] ** 2).sum()\n",
        "\n",
        "  reg = l_reg * ((bu**2).sum() + (bi**2).sum())  \n",
        "  loss += reg\n",
        "\n",
        "  return loss\n",
        "\n",
        "\n",
        "def baseline_estimator(mat_file, l_reg=0.02, learning_rate=0.0000025):\n",
        "\n",
        "  mat = io.loadmat(mat_file)['X']\n",
        "  \"\"\"mat = mat[1:5000,1:5000]\n",
        "  mat = mat[mat.getnnz(1)>0][:,mat.getnnz(0)>0]\"\"\"\n",
        "\n",
        "  print(mat.shape)\n",
        "  no_users = mat.shape[0]\n",
        "  no_movies = mat.shape[1]\n",
        "  \n",
        "  bu = np.random.rand(no_users,1)  * 2 - 1\n",
        "  bi = np.random.rand(1,no_movies) * 2 - 1\n",
        "  #bu = np.zeros((no_users,1))\n",
        "  #bi = np.zeros((1,no_movies))  \n",
        "\n",
        "  mu = mat.data[:].mean()\n",
        "  mat_sum1 = mat.sum(1)\n",
        "  mat_sum0 = mat.sum(0)\n",
        "  n = mat.data[:].shape[0]\n",
        "\n",
        "  no_users_entries = np.array((mat != 0).sum(1))\n",
        "  no_movies_entries = np.array((mat != 0).sum(0))\n",
        "\n",
        "  # Train\n",
        "  print(\"Train...\")\n",
        "  n_iter = 200\n",
        "  for it in range(n_iter):\n",
        "\n",
        "    #bi_sum = bi[bi_index].sum(1).reshape((no_users,1))\n",
        "    #bu_sum = bu.ravel()[bu_index].sum(0).reshape((1,no_movies)) \n",
        "\n",
        "    bi_sum = np.array(list(map(lambda x:bi.ravel()[x].sum(), bi_index))).reshape((no_users,1))\n",
        "    bu_sum = np.array(list(map(lambda x:bu.ravel()[x].sum(), bu_index))).reshape((1,no_movies))    \n",
        "\n",
        "    bu_gradient = - 2.0 * (mat_sum1 - no_users_entries  * mu - no_users_entries  * bu - bi_sum) + 2.0 * l_reg * bu\n",
        "    bu -= learning_rate * bu_gradient \n",
        "\n",
        "    bi_gradient = - 2.0 * (mat_sum0 - no_movies_entries * mu - no_movies_entries * bi - bu_sum) + 2.0 * l_reg * bi\n",
        "    bi -= learning_rate * bi_gradient \n",
        " \n",
        "    \"\"\"print(bu.mean())    \n",
        "    print(bi.mean())    \n",
        "    print(bu_gradient.mean())\n",
        "    print(bi_gradient.mean())\"\"\"\n",
        "    if it % 10 == 0:\n",
        "      print(it, \"\\ \", n_iter)         \n",
        "      \"\"\"print(bu)\n",
        "      print(bi)      \n",
        "      print(bu_gradient)\n",
        "      print(bi_gradient)\"\"\"\n",
        "      print(\"compute loss...\")\n",
        "      print(compute_loss(mat, mu, bu, bi, l_reg=l_reg))\n",
        "\n",
        "  return bu, bi\n",
        "#################################################\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  \n",
        "  bu_index, bi_index = pre_processing()\n",
        "  baseline_estimator(path+\"/T.mat\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFXT6MsO7Wrk",
        "colab_type": "text"
      },
      "source": [
        "### **2. Correlation-Based Neighbourhood Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gGumOSo7rKa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0mFO-G57ZLP",
        "colab_type": "text"
      },
      "source": [
        "### **3. Correlation-Based Neighbourhood Model with Implicit Feedback**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7DjKnRT7rxM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPFCaV067d0y",
        "colab_type": "text"
      },
      "source": [
        "### **4. SVD++**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKG3HgK98Gym",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94Hh0sex77yF",
        "colab_type": "text"
      },
      "source": [
        "### **5. Integrated Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkJ8byiA8H0W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}