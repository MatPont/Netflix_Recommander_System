{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AFMatriciel_Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "mX5jkgnd6qwn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSMDkhRuujbY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy import io\n",
        "\n",
        "mat = io.loadmat(path+\"/D.mat\")['X']\n",
        "print(\"D\")\n",
        "print(mat.shape)\n",
        "\n",
        "mat = io.loadmat(path+\"/T.mat\")['X']\n",
        "print(\"T\")\n",
        "print(mat.shape)\n",
        "\n",
        "mat = io.loadmat(path+\"/R.mat\")['X']\n",
        "print(\"R\")\n",
        "print(mat.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nk6M591C6y8g",
        "colab_type": "text"
      },
      "source": [
        "### **Data Compilation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEF8agxz6rhh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import os\n",
        "from scipy.sparse import dok_matrix, csr_matrix\n",
        "from scipy import io\n",
        "import tarfile\n",
        "\n",
        "\n",
        "\n",
        "#################################################\n",
        "total_no_users = 2649429\n",
        "total_no_movies = 17770\n",
        "\n",
        "def process_content(content, D):\n",
        "    lines = content.split(\"\\n\")\n",
        "    id_movie = int(lines[0][:-1]) - 1\n",
        "    for i in range(1, len(lines)):\n",
        "        if lines[i] != '':\n",
        "            line = lines[i].split(\",\")\n",
        "            id_user = int(line[0]) - 1\n",
        "            rating = int(line[1])\n",
        "            D[id_user, id_movie] = rating\n",
        "    return D\n",
        "\n",
        "\n",
        "def rating_compiler(folder_name, out_path):\n",
        "    D = dok_matrix((total_no_users, total_no_movies))\n",
        "    res_listdir = os.listdir(folder_name)\n",
        "    number = len(res_listdir)\n",
        "    i = 0\n",
        "    for f in res_listdir:\n",
        "        if os.path.isfile(folder_name+f):\n",
        "            print(i, \" / \", number)\n",
        "            myfile = open(folder_name+f)\n",
        "            content = myfile.read()\n",
        "            myfile.close()\n",
        "            D = process_content(content, D)\n",
        "        i += 1\n",
        "    D = csr_matrix(D)             \n",
        "    io.savemat(out_path, {'X' : D})\n",
        "\n",
        "\n",
        "def rating_compiler2(tar_name, out_path):\n",
        "    D = dok_matrix((total_no_users, total_no_movies))\n",
        "    tar = tarfile.open(tar_name)\n",
        "    res_getmembers = tar.getmembers()\n",
        "    number = len(res_getmembers)\n",
        "    i = 0\n",
        "    for member in res_getmembers:\n",
        "        f = tar.extractfile(member)\n",
        "        if f is not None:    \n",
        "            print(i, \" / \", number)        \n",
        "            content = f.read()\n",
        "            f.close()\n",
        "            D = process_content(content.decode(), D)\n",
        "        i += 1\n",
        "    tar.close()\n",
        "    D = csr_matrix(D)             \n",
        "    io.savemat(out_path, {'X' : D})\n",
        "\n",
        "\n",
        "def extract_T_and_R(D_file_name, file_name, out_T_path, out_R_path):\n",
        "    D = io.loadmat(D_file_name)['X']\n",
        "    myfile = open(file_name)\n",
        "    content = myfile.read()\n",
        "    myfile.close()\n",
        "    lines = content.split(\"\\n\")\n",
        "    users, movies = set(), set()\n",
        "    for line in lines:\n",
        "        if line != '':\n",
        "            line_split = line.split(\":\")\n",
        "            if len(line_split) == 2:\n",
        "                # Movie id\n",
        "                movies.add(int(line_split[0]) - 1)\n",
        "            else:\n",
        "                # User id\n",
        "                users.add(int(line_split[0]) - 1)\n",
        "    T = D[list(users),:]\n",
        "    T = T[:,list(movies)]    \n",
        "    io.savemat(out_T_path, {'X' : T})\n",
        "    \n",
        "    movies2 = set(range(total_no_movies))\n",
        "    movies2 = movies2.difference(movies)\n",
        "    users2 = set(range(total_no_users))\n",
        "    users2 = users2.difference(users)\n",
        "    \n",
        "    R = D[list(users2),:]\n",
        "    R = R[:,list(movies2)]\n",
        "    io.savemat(out_R_path, {'X' : R})\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "#################################################\n",
        "if __name__ == \"__main__\":\n",
        "    #rating_compiler2(path+\"/download/training_set.tar\", path+\"/D.mat\")\n",
        "    #extract_T_and_R(path+\"/D.mat\", path+\"/download/qualifying.txt\", path+\"/T.mat\", path+\"/R.mat\")\n",
        "    extract_T_and_R(path+\"/D.mat\", path+\"/download/probe.txt\", path+\"/T.mat\", path+\"/R.mat\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dX0uFBwWNQbC",
        "colab_type": "text"
      },
      "source": [
        "### **Utils**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmoTi3QCNUzr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from itertools import groupby\n",
        "from operator import itemgetter\n",
        "import pickle\n",
        "import os\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "path = \"/content/drive/My Drive/M2/AFMatriciel\"\n",
        "\n",
        "def compute_sparse_correlation_matrix(A):\n",
        "    scaler = StandardScaler(with_mean=False)\n",
        "    scaled_A = scaler.fit_transform(A)  # Assuming A is a CSR or CSC matrix\n",
        "    corr_matrix = (1/scaled_A.shape[0]) * (scaled_A.T @ scaled_A)\n",
        "    return corr_matrix\n",
        "\n",
        "def pre_processing(mat, mat_file):\n",
        "    # Create bu and bi indexes\n",
        "    # bi_index is a list with a size equal to the number of users\n",
        "    #    the jth element is a list storing the indexes of movies rated by user j\n",
        "    # bu_index is the same but storing the indexes of users whose rating is \n",
        "    #    available for a movie\n",
        "    # These indexes will help to vectorize computation of the gradient\n",
        "\n",
        "    shape = str(mat.shape[0])+\"_\"+str(mat.shape[1])\n",
        "    bu_index_file = mat_file+\"_bu_index_\"+shape+\".data\"\n",
        "    bi_index_file = mat_file+\"_bi_index_\"+shape+\".data\"\n",
        "\n",
        "    if not (os.path.isfile(bu_index_file) and os.path.isfile(bi_index_file)):\n",
        "        #mat = io.loadmat(mat_file)['X']\n",
        "        \"\"\"mat = mat[1:5000,1:5000]\n",
        "        mat = mat[mat.getnnz(1)>0][:,mat.getnnz(0)>0]\"\"\"\n",
        "\n",
        "        print(\"Pre-processing...\")\n",
        "        mat_nonzero = mat.nonzero()\n",
        "        \"\"\"cx = mat.tocoo()    \n",
        "        bi_index = [[]]*mat.shape[0]\n",
        "        bu_index = [[]]*mat.shape[1]\n",
        "        for i,j,v in zip(cx.row, cx.col, cx.data):\n",
        "          bi_index[i].append(j)\n",
        "          bu_index[j].append(i)\n",
        "        print(bi_index[0])\"\"\"\n",
        "\n",
        "        print(\"   make bi indexes...\")\n",
        "        bi_index = []\n",
        "        for k, g in groupby(zip(mat_nonzero[0], mat_nonzero[1]), itemgetter(0)):\n",
        "          to_add = list(map(lambda x:int(x[1]), list(g)))\n",
        "          bi_index.append(to_add)\n",
        "\n",
        "        print(\"   make bu indexes...\")\n",
        "        bu_index = []\n",
        "        indexes = np.argsort(mat_nonzero[1])\n",
        "        for k, g in groupby(zip(mat_nonzero[1][indexes], mat_nonzero[0][indexes]), itemgetter(0)):\n",
        "          to_add = list(map(lambda x:int(x[1]), list(g)))\n",
        "          bu_index.append(to_add)    \n",
        "\n",
        "        with open(bi_index_file, \"wb\") as fp:\n",
        "            pickle.dump(bi_index, fp)\n",
        "        with open(bu_index_file, \"wb\") as fp:\n",
        "            pickle.dump(bu_index, fp)\n",
        "    else:\n",
        "        with open(bi_index_file, \"rb\") as fp:\n",
        "            bi_index = pickle.load(fp)\n",
        "        with open(bu_index_file, \"rb\") as fp:\n",
        "            bu_index = pickle.load(fp)\n",
        "\n",
        "    print(\"Pre-processing done.\")\n",
        "    return bu_index, bi_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NN61ajtw7D0C",
        "colab_type": "text"
      },
      "source": [
        "### **1. Baseline Estimates**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5AUhGWy7qR1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy import io, sparse\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "#################################################\n",
        "def compute_loss(mat, mu, bu, bi, l_reg=0.02):\n",
        "  loss = 0\n",
        "\n",
        "  no_users_entries = np.array((mat != 0).sum(1)).T.ravel()\n",
        "  bu_rep = np.repeat(bu.ravel(), no_users_entries)\n",
        "\n",
        "  no_movies_entries = np.array((mat != 0).sum(0)).ravel()\n",
        "  bi_rep = np.repeat(bi.ravel(), no_movies_entries)\n",
        "\n",
        "  temp_mat = sparse.csc_matrix(mat).copy()\n",
        "  temp_mat.data[:] -= bi_rep\n",
        "  temp_mat.data[:] -= mu\n",
        "  temp_mat = sparse.coo_matrix(temp_mat)\n",
        "  temp_mat = sparse.csr_matrix(temp_mat)\n",
        "  temp_mat.data[:] -= bu_rep\n",
        "\n",
        "  loss = (temp_mat.data[:] ** 2).sum()\n",
        "\n",
        "  reg = l_reg * ((bu**2).sum() + (bi**2).sum())  \n",
        "  loss += reg\n",
        "\n",
        "  return loss\n",
        "\n",
        "\n",
        "def baseline_estimator(mat, mat_file, l_reg=0.02, learning_rate=0.0000025):\n",
        "  # subsample the matrix to make computation faster\n",
        "  \"\"\"mat = mat[0:mat.shape[0]//128, 0:mat.shape[1]//128]\n",
        "  mat = mat[mat.getnnz(1)>0][:, mat.getnnz(0)>0]\"\"\"\n",
        "\n",
        "  print(mat.shape)\n",
        "  no_users = mat.shape[0]\n",
        "  no_movies = mat.shape[1]\n",
        "  \n",
        "  bu_index, bi_index = pre_processing(mat, mat_file)\n",
        "\n",
        "  bu = np.random.rand(no_users,1)  * 2 - 1\n",
        "  bi = np.random.rand(1,no_movies) * 2 - 1\n",
        "  #bu = np.zeros((no_users,1))\n",
        "  #bi = np.zeros((1,no_movies))  \n",
        "\n",
        "  mu = mat.data[:].mean()\n",
        "  mat_sum1 = mat.sum(1)\n",
        "  mat_sum0 = mat.sum(0)\n",
        "  n = mat.data[:].shape[0]\n",
        "\n",
        "  no_users_entries = np.array((mat != 0).sum(1))\n",
        "  no_movies_entries = np.array((mat != 0).sum(0))\n",
        "\n",
        "  # Train\n",
        "  print(\"Train...\")\n",
        "  n_iter = 200\n",
        "  for it in range(n_iter):\n",
        "\n",
        "    #bi_sum = bi[bi_index].sum(1).reshape((no_users,1))\n",
        "    #bu_sum = bu.ravel()[bu_index].sum(0).reshape((1,no_movies)) \n",
        "\n",
        "    bi_sum = np.array(list(map(lambda x:bi.ravel()[x].sum(), bi_index))).reshape((no_users,1))\n",
        "    bu_sum = np.array(list(map(lambda x:bu.ravel()[x].sum(), bu_index))).reshape((1,no_movies))    \n",
        "\n",
        "    # Vectorized operations\n",
        "    bu_gradient = - 2.0 * (mat_sum1 - no_users_entries  * mu - no_users_entries  * bu - bi_sum) + 2.0 * l_reg * bu\n",
        "    bu -= learning_rate * bu_gradient \n",
        "\n",
        "    bi_gradient = - 2.0 * (mat_sum0 - no_movies_entries * mu - no_movies_entries * bi - bu_sum) + 2.0 * l_reg * bi\n",
        "    bi -= learning_rate * bi_gradient \n",
        " \n",
        "    if it % 10 == 0:\n",
        "      print(\"compute loss...\")\n",
        "      print(compute_loss(mat, mu, bu, bi, l_reg=l_reg))\n",
        "\n",
        "  return bu, bi\n",
        "#################################################\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    mat_file = path+\"/T.mat\"\n",
        "    mat = io.loadmat(mat_file)['X']    \n",
        "    baseline_estimator(mat, mat_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFXT6MsO7Wrk",
        "colab_type": "text"
      },
      "source": [
        "### **2. Correlation-Based Neighbourhood Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gGumOSo7rKa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from scipy import io, sparse\n",
        "from math import sqrt\n",
        "from time import time\n",
        "\n",
        "\n",
        "\n",
        "# Through all this code Rk_iu and Nk_iu are the same since implicit matrix is\n",
        "#    made from the rating matrix without additional information (i.e. indexes of\n",
        "#    non-zero elements are the same therefore neighbors too).\n",
        "\n",
        "\n",
        "#################################################\n",
        "# Non-vectorized way (iterate through each r_ui)\n",
        "#################################################\n",
        "def predict_r_ui(mat, u, i, mu, S, Sk_iu, baseline_bu, baseline_bi):\n",
        "  bui = mu + baseline_bu[u] + baseline_bi[0, i]\n",
        "  buj = mu + baseline_bu[u] + baseline_bi[0, Sk_iu]\n",
        "  return bui + 1 / S[i, Sk_iu].sum() * (S[i, Sk_iu].toarray().ravel() * (mat[u, Sk_iu].toarray().ravel() - buj)).sum()\n",
        "\n",
        "def correlation_based_neighbourhood_model(mat, mat_file, l_reg2=100.0, k=250):\n",
        "    # subsample the matrix to make computation faster\n",
        "    \"\"\"mat = mat[0:mat.shape[0]//128, 0:mat.shape[1]//128]\n",
        "    mat = mat[mat.getnnz(1)>0][:, mat.getnnz(0)>0]\"\"\"\n",
        "\n",
        "    print(mat.shape)\n",
        "    no_users = mat.shape[0]\n",
        "    no_movies = mat.shape[1]\n",
        "\n",
        "    #baseline_bu, baseline_bi = baseline_estimator(mat)\n",
        "    # We should call baseline_estimator but we can init at random for test\n",
        "    baseline_bu, baseline_bi = np.random.rand(no_users, 1)  * 2 - 1, np.random.rand(1, no_movies) * 2 - 1    \n",
        "\n",
        "    #bu_index, bi_index = pre_processing(mat, mat_file)\n",
        "\n",
        "    mu = mat.data[:].mean()\n",
        "\n",
        "    # Compute similarity matrix\n",
        "    N = sparse.csr_matrix(mat).copy()\n",
        "    N.data[:] = 1\n",
        "    S = sparse.csr_matrix.dot(N.T, N)\n",
        "    S.data[:] = S.data[:] / (S.data[:] + l_reg2)\n",
        "    S = S * compute_sparse_correlation_matrix(mat)\n",
        "\n",
        "    # Computation\n",
        "    print(\"Computation...\")\n",
        "    n_iter = 200\n",
        "    cx = mat.tocoo()\n",
        "    r_ui_mat = []\n",
        "    for u,i,v in zip(cx.row, cx.col, cx.data):\n",
        "        Sk_iu = np.flip(np.argsort(S[i,].toarray()))[:k].ravel()\n",
        "        r_ui = predict_r_ui(mat, u, i, mu, S, Sk_iu, baseline_bu, baseline_bi)\n",
        "        r_ui_mat.append((u, i, r_ui))\n",
        "\n",
        "    return r_ui_mat\n",
        "\n",
        "#################################################\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    mat_file = path+\"/T.mat\"\n",
        "    mat = io.loadmat(mat_file)['X']\n",
        "    correlation_based_neighbourhood_model(mat, mat_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0mFO-G57ZLP",
        "colab_type": "text"
      },
      "source": [
        "### **3. Correlation-Based Neighbourhood Model with Implicit Feedback**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7DjKnRT7rxM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from scipy import io, sparse\n",
        "from math import sqrt\n",
        "from time import time\n",
        "\n",
        "\n",
        "\n",
        "# Through all this code Rk_iu and Nk_iu are the same since implicit matrix is\n",
        "#    made from the rating matrix without additional information (i.e. indexes of\n",
        "#    non-zero elements are the same therefore neighbors too).\n",
        "\n",
        "\n",
        "#################################################\n",
        "# Non-vectorized way (iterate through each r_ui)\n",
        "#################################################\n",
        "def predict_r_ui(mat, u, i, mu, bu, bi, Rk_iu, wij, Nk_iu, cij, baseline_bu, baseline_bi):\n",
        "    buj = mu + baseline_bu[u] + baseline_bi[0, Rk_iu]\n",
        "    Rk_iu_sum = np.multiply((mat[u, Rk_iu] - buj), wij[i][Rk_iu]).sum()\n",
        "    Nk_iu_sum = cij[i][Rk_iu].sum()\n",
        "    return mu + bu[u] + bi[0, i] + Rk_iu_sum / sqrt(len(Rk_iu)) + Nk_iu_sum / sqrt(len(Nk_iu))\n",
        "\n",
        "def compute_e_ui(mat, u, i, mu, bu, bi, Rk_iu, wij, Nk_iu, cij, baseline_bu, baseline_bi):\n",
        "    return mat[u, i] - predict_r_ui(mat, u, i, mu, bu, bi, Rk_iu, wij, Nk_iu, cij, baseline_bu, baseline_bi)\n",
        "\n",
        "def compute_loss(mat, mu, bu, bi, Rk_iu, wij, Nk_iu, cij, baseline_bu, baseline_bi, l_reg=0.002):\n",
        "    loss = 0\n",
        "    cx = mat.tocoo()        \n",
        "    for u,i,v in zip(cx.row, cx.col, cx.data):\n",
        "        r_ui_pred = predict_r_ui(mat, u, i, mu, bu, bi, Rk_iu, wij, Nk_iu, cij, baseline_bu, baseline_bi)\n",
        "        Rk_iu_sum = (wij[i][Rk_iu] ** 2).sum()\n",
        "        Nk_iu_sum = (cij[i][Rk_iu] ** 2).sum()\n",
        "        loss += (mat[u, i] - r_ui_pred) ** 2 + l_reg * ((bu ** 2).sum() + (bi ** 2).sum() + Rk_iu_sum + Nk_iu_sum) \n",
        "\n",
        "    return loss\n",
        "\n",
        "def correlation_based_implicit_neighbourhood_model(mat, mat_file, l_reg=0.002, gamma=0.005, l_reg2=100.0, k=250):\n",
        "    # subsample the matrix to make computation faster\n",
        "    \"\"\"mat = mat[0:mat.shape[0]//128, 0:mat.shape[1]//128]\n",
        "    mat = mat[mat.getnnz(1)>0][:, mat.getnnz(0)>0]\"\"\"\n",
        "\n",
        "    print(mat.shape)\n",
        "    no_users = mat.shape[0]\n",
        "    no_movies = mat.shape[1]\n",
        "\n",
        "    #baseline_bu, baseline_bi = baseline_estimator(mat)\n",
        "    # We should call baseline_estimator but we can init at random for test\n",
        "    baseline_bu, baseline_bi = np.random.rand(no_users, 1)  * 2 - 1, np.random.rand(1, no_movies) * 2 - 1    \n",
        "\n",
        "    bu_index, bi_index = pre_processing(mat, mat_file)\n",
        "    \n",
        "    # Init parameters\n",
        "    bu = np.random.rand(no_users, 1)  * 2 - 1\n",
        "    bi = np.random.rand(1, no_movies) * 2 - 1\n",
        "    wij = np.random.rand(no_movies, no_movies) * 2 - 1\n",
        "    cij = np.random.rand(no_movies, no_movies) * 2 - 1\n",
        "\n",
        "    mu = mat.data[:].mean()\n",
        "\n",
        "    # Compute similarity matrix\n",
        "    N = sparse.csr_matrix(mat).copy()\n",
        "    N.data[:] = 1\n",
        "    S = sparse.csr_matrix.dot(N.T, N)\n",
        "    S.data[:] = S.data[:] / (S.data[:] + l_reg2)\n",
        "    S = S * compute_sparse_correlation_matrix(mat)\n",
        "\n",
        "    # Train\n",
        "    print(\"Train...\")\n",
        "    n_iter = 200\n",
        "    cx = mat.tocoo()        \n",
        "    for it in range(n_iter):\n",
        "        t0 = time()\n",
        "        for u,i,v in zip(cx.row, cx.col, cx.data):\n",
        "            #Rk_iu = Nk_iu = bi_index[u]\n",
        "            Rk_iu = Nk_iu = np.flip(np.argsort(S[i,].toarray()))[:k].ravel()\n",
        "            e_ui = compute_e_ui(mat, u, i, mu, bu, bi, Rk_iu, wij, Nk_iu, cij, baseline_bu, baseline_bi)\n",
        "\n",
        "            bu[u] += gamma * (e_ui - l_reg * bu[u])\n",
        "            bi[0, i] += gamma * (e_ui - l_reg * bi[0, i])\n",
        "\n",
        "            buj = mu + baseline_bu[u] + baseline_bi[0, Rk_iu]\n",
        "            wij[i][Rk_iu] += gamma * ( 1 / sqrt(len(Rk_iu)) * e_ui * (mat[u, Rk_iu].toarray().ravel() - buj) - l_reg * wij[i][Rk_iu] )\n",
        "            cij[i][Nk_iu] += gamma * ( 1 / sqrt(len(Nk_iu)) * e_ui - l_reg * cij[i][Nk_iu] )\n",
        "        gamma *= 0.99\n",
        "\n",
        "        if it % 10 == 0:\n",
        "          t1 = time()\n",
        "          print(it, \"\\ \", n_iter, \"(%.2g sec)\" % (t1 - t0))\n",
        "          print(\"compute loss...\")\n",
        "          print(compute_loss(mat, mu, bu, bi, Rk_iu, wij, Nk_iu, cij, baseline_bu, baseline_bi, l_reg=l_reg))\n",
        "\n",
        "    return bu, bi, wij, cij\n",
        "#################################################\n",
        "\n",
        "\n",
        "#################################################\n",
        "# Vectorized way (in work)\n",
        "# (Actually this version is faster but updates e_ui\n",
        "# less frequently making it less accurate for the\n",
        "# gradient descent)\n",
        "#################################################\n",
        "def compute_e_vectorized(mat, mu, bu, bi, Rk, wij, Nk, cij, baseline_bu, baseline_bi):\n",
        "    # Rk and Nk are list of tuple (u, i, Rk_iu/Nk_iu)\n",
        "\n",
        "    no_users_entries = np.array((mat != 0).sum(1)).T.ravel()\n",
        "    bu_rep = np.repeat(bu.ravel(), no_users_entries)\n",
        "\n",
        "    no_movies_entries = np.array((mat != 0).sum(0)).ravel()\n",
        "    bi_rep = np.repeat(bi.ravel(), no_movies_entries)\n",
        "\n",
        "    temp_mat = sparse.csc_matrix(mat).copy()\n",
        "    temp_mat.data[:] -= mu\n",
        "    temp_mat.data[:] -= bi_rep\n",
        "    Rk_sum = np.array(list(map(lambda x : ( (mat[x[0], x[2]].toarray().ravel() \\\n",
        "                                           - (mu + baseline_bu[x[0]] + baseline_bi[0, x[2]])) \\\n",
        "                                           * wij[x[1]][x[2]] ).sum() / sqrt(len(x[2])), Rk)))\n",
        "    temp_mat.data[:] -= Rk_sum\n",
        "    Nk_sum = np.array(list(map(lambda x : cij[x[1]][x[2]].sum() / sqrt(len(x[2])), Nk)))\n",
        "    temp_mat.data[:] -= Nk_sum\n",
        "    temp_mat = sparse.coo_matrix(temp_mat)\n",
        "    temp_mat = sparse.csr_matrix(temp_mat)\n",
        "    temp_mat.data[:] -= bu_rep\n",
        "\n",
        "    return temp_mat\n",
        "\n",
        "def compute_loss_vectorized(mat, mu, bu, bi, Rk, wij, Nk, cij, baseline_bu, baseline_bi, l_reg=0.002):\n",
        "    no_nonzero_element = np.array((mat != 0).sum())\n",
        "    loss = (compute_e_vectorized(mat, mu, bu, bi, Rk, wij, Nk, cij, baseline_bu, baseline_bi).data[:] ** 2).sum()\n",
        "    loss += l_reg * np.array(list(map(lambda x : (cij[x[1]][x[2]] ** 2).sum(), Nk))).sum()\n",
        "    loss += l_reg * np.array(list(map(lambda x : (wij[x[1]][x[2]] ** 2).sum(), Rk))).sum()\n",
        "    loss += no_nonzero_element * l_reg * (bu ** 2).sum()\n",
        "    loss += no_nonzero_element * l_reg * (bi ** 2).sum()\n",
        "\n",
        "    return loss\n",
        "\n",
        "def correlation_based_implicit_neighbourhood_model_vectorized(mat, mat_file, l_reg=0.002, gamma=0.005, l_reg2=100.0, k=250):\n",
        "    gamma /= 100\n",
        "\n",
        "    # subsample the matrix to make computation faster\n",
        "    mat = mat[0:mat.shape[0]//128, 0:mat.shape[1]//128]\n",
        "    mat = mat[mat.getnnz(1)>0][:, mat.getnnz(0)>0]\n",
        "\n",
        "    print(mat.shape)\n",
        "    no_users = mat.shape[0]\n",
        "    no_movies = mat.shape[1]\n",
        "    no_users_entries = np.array((mat != 0).sum(1))\n",
        "    no_movies_entries = np.array((mat != 0).sum(0))    \n",
        "\n",
        "    #baseline_bu, baseline_bi = baseline_estimator(mat)\n",
        "    # We should call baseline_estimator but we can init at random for testing\n",
        "    baseline_bu, baseline_bi = np.random.rand(no_users, 1)  * 2 - 1, np.random.rand(1, no_movies) * 2 - 1    \n",
        "\n",
        "    bu_index, bi_index = pre_processing(mat, mat_file)\n",
        "    \n",
        "    bu = np.random.rand(no_users, 1)  * 2 - 1\n",
        "    bi = np.random.rand(1, no_movies) * 2 - 1\n",
        "    wij = np.random.rand(no_movies, no_movies) * 2 - 1\n",
        "    cij = np.random.rand(no_movies, no_movies) * 2 - 1\n",
        "\n",
        "    mu = mat.data[:].mean()\n",
        "\n",
        "    # Compute similarity matrix\n",
        "    N = sparse.csr_matrix(mat).copy()\n",
        "    N.data[:] = 1\n",
        "    S = sparse.csr_matrix.dot(N.T, N)\n",
        "    S.data[:] = S.data[:] / (S.data[:] + l_reg2)\n",
        "    S = S * compute_sparse_correlation_matrix(mat)\n",
        "\n",
        "    Rk = []\n",
        "    cx = mat.tocoo()\n",
        "    for u,i,v in zip(cx.row, cx.col, cx.data):\n",
        "        Rk.append((u, i, np.flip(np.argsort(S[i,].toarray()))[:k].ravel()))\n",
        "\n",
        "    # Train\n",
        "    print(\"Train...\")\n",
        "    n_iter = 200\n",
        "    for it in range(n_iter):\n",
        "        t0 = time() \n",
        "\n",
        "        e = compute_e_vectorized(mat, mu, bu, bi, Rk, wij, Rk, cij, baseline_bu, baseline_bi)\n",
        "        # Vectorized operations\n",
        "        bu += gamma * (e.sum(1) - no_users_entries * l_reg * bu)\n",
        "        bi += gamma * (e.sum(0) - no_movies_entries * l_reg * bi)\n",
        "\n",
        "        # TODO: vectorize the following\n",
        "        for u, i, Rk_iu in Rk:\n",
        "            Nk_iu = Rk_iu\n",
        "            e_ui = e[u, i]\n",
        "            buj = mu + baseline_bu[u] + baseline_bi[0, Rk_iu]\n",
        "            wij[i][Rk_iu] += gamma * ( 1 / sqrt(len(Rk_iu)) * e_ui * (mat[u, Rk_iu].toarray().ravel() - buj) - l_reg * wij[i][Rk_iu] )\n",
        "            cij[i][Nk_iu] += gamma * ( 1 / sqrt(len(Nk_iu)) * e_ui - l_reg * cij[i][Nk_iu] )\n",
        "        gamma *= 0.99\n",
        "\n",
        "        if it % 10 == 0:\n",
        "          t1 = time()\n",
        "          print(it, \"\\ \", n_iter, \"(%.2g sec)\" % (t1 - t0))       \n",
        "          print(\"compute loss...\")\n",
        "          print(compute_loss_vectorized(mat, mu, bu, bi, Rk, wij, Rk, cij, baseline_bu, baseline_bi, l_reg=l_reg))  \n",
        "\n",
        "    return bu, bi, wij, cij\n",
        "#################################################\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    mat_file = path+\"/T.mat\"\n",
        "    mat = io.loadmat(mat_file)['X']\n",
        "    correlation_based_implicit_neighbourhood_model(mat, mat_file)\n",
        "    #correlation_based_implicit_neighbourhood_model_vectorized(mat, mat_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRshPEyLO2rm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy import io\n",
        "import numpy as np\n",
        "mat = io.loadmat(path+\"/T.mat\")['X']\n",
        "\n",
        "no_users = mat.shape[0]\n",
        "no_movies = mat.shape[1]\n",
        "bu = np.random.rand(no_users, 1)  * 2 - 1\n",
        "bi = np.random.rand(1, no_movies) * 2 - 1\n",
        "\n",
        "print(bi.shape)\n",
        "print(bi[0].shape)\n",
        "print(bi[0, 0].shape)\n",
        "\n",
        "\"\"\"a = np.corrcoef(mat)\n",
        "print(a)\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPFCaV067d0y",
        "colab_type": "text"
      },
      "source": [
        "### **4. SVD++**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKG3HgK98Gym",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from scipy import io, sparse\n",
        "from math import sqrt, isnan\n",
        "\n",
        "\n",
        "\n",
        "#################################################\n",
        "# Non-vectorized way\n",
        "#################################################\n",
        "def predict_r_ui(mat, u, i, mu, bu, bi, qi, pu, N_u, yj):\n",
        "    N_u_sum = yj[N_u].sum(0)\n",
        "    return mu + bu[u] + bi[0, i] + np.dot(qi[i], (pu[u] + N_u_sum / sqrt(len(N_u))))\n",
        "\n",
        "def compute_e_ui(mat, u, i, mu, bu, bi, qi, pu, N_u, yj):\n",
        "    return mat[u, i] - predict_r_ui(mat, u, i, mu, bu, bi, qi, pu, N_u, yj)\n",
        "\n",
        "def compute_loss(mat, mu, bu, bi, qi, pu, N_u, yj, l_reg6=0.005, l_reg7=0.015):\n",
        "    loss = 0\n",
        "    cx = mat.tocoo()\n",
        "    for u,i,v in zip(cx.row, cx.col, cx.data):\n",
        "        r_ui_pred = predict_r_ui(mat, u, i, mu, bu, bi, qi, pu, N_u, yj)\n",
        "        loss += (mat[u, i] - r_ui_pred) ** 2 + l_reg6 * ((bu ** 2).sum() + (bi ** 2).sum())\n",
        "        loss += l_reg7 * ((qi[i]**2).sum() + (pu[u]**2).sum() + (yj[N_u]**2).sum())\n",
        "\n",
        "    return loss\n",
        "\n",
        "def svd_more_more(mat, mat_file, gamma1=0.007, gamma2=0.007, gamma3=0.001, l_reg2=100, l_reg6=0.005, l_reg7=0.015, f=50):\n",
        "    # subsample the matrix to make computation faster\n",
        "    \"\"\"mat = mat[0:mat.shape[0]//128, 0:mat.shape[1]//128]\n",
        "    mat = mat[mat.getnnz(1)>0][:, mat.getnnz(0)>0]\"\"\"\n",
        "\n",
        "    print(mat.shape)\n",
        "    no_users = mat.shape[0]\n",
        "    no_movies = mat.shape[1]\n",
        "\n",
        "    bu_index, bi_index = pre_processing(mat, mat_file)\n",
        "    \n",
        "    # Init parameters\n",
        "    bu = np.random.rand(no_users, 1)  * 2 - 1\n",
        "    bi = np.random.rand(1, no_movies) * 2 - 1\n",
        "    qi = np.random.rand(no_movies, f) * 2 - 1\n",
        "    pu = np.random.rand(no_users, f) * 2 - 1\n",
        "    yj = np.random.rand(no_movies, f) * 2 - 1\n",
        "\n",
        "    mu = mat.data[:].mean()\n",
        "\n",
        "    # Train\n",
        "    print(\"Train...\")\n",
        "    n_iter = 200\n",
        "    cx = mat.tocoo()\n",
        "    for it in range(n_iter):\n",
        "        for u,i,v in zip(cx.row, cx.col, cx.data):\n",
        "            N_u = bi_index[u]\n",
        "            e_ui = compute_e_ui(mat, u, i, mu, bu, bi, qi, pu, N_u, yj)\n",
        "\n",
        "            bu[u] += gamma1 * (e_ui - l_reg6 * bu[u])\n",
        "            bi[0, i] += gamma1 * (e_ui - l_reg6 * bi[0, i])\n",
        "            qi[i] += gamma2 * (e_ui * (pu[u] + 1 / sqrt(len(N_u)) * yj[N_u].sum(0)) - l_reg7 * qi[i])\n",
        "            pu[u] += gamma2 * (e_ui * qi[i] - l_reg7 * pu[u])\n",
        "            yj[N_u] += gamma2 * (e_ui * 1/ sqrt(len(N_u)) * qi[i] - l_reg7 * yj[N_u])\n",
        "        gamma1 *= 0.9\n",
        "        gamma2 *= 0.9\n",
        "\n",
        "        if it % 10 == 0:\n",
        "          print(it, \"\\ \", n_iter)         \n",
        "          print(\"compute loss...\")\n",
        "          print(compute_loss(mat, mu, bu, bi, qi, pu, N_u, yj, l_reg6=l_reg6, l_reg7=l_reg7))\n",
        "    \n",
        "    return bu, bi, qi, pu, yj\n",
        "#################################################\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    mat_file = path+\"/T.mat\"\n",
        "    mat = io.loadmat(mat_file)['X']\n",
        "    svd_more_more(mat, mat_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94Hh0sex77yF",
        "colab_type": "text"
      },
      "source": [
        "### **5. Integrated Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkJ8byiA8H0W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from scipy import io, sparse\n",
        "from math import sqrt, isnan\n",
        "\n",
        "\n",
        "\n",
        "# Through all this code Rk_iu and Nk_iu are the same since implicit matrix is\n",
        "#    made from the rating matrix without additional information.\n",
        "\n",
        "\n",
        "#################################################\n",
        "# Non-vectorized way\n",
        "#################################################\n",
        "def predict_r_ui(mat, u, i, mu, bu, bi, Rk_iu, wij, Nk_iu, cij, baseline_bu, baseline_bi, qi, pu, N_u, yj):\n",
        "    buj = mu + baseline_bu[u] + baseline_bi[0, Rk_iu]\n",
        "    Rk_iu_sum = np.multiply((mat[u, Rk_iu] - buj), wij[i][Rk_iu]).sum()\n",
        "    Nk_iu_sum = cij[i][Rk_iu].sum()\n",
        "    N_u_sum = yj[N_u].sum(0)\n",
        "    return mu + bu[u] + bi[0, i] + np.dot(qi[i], (pu[u] + N_u_sum / sqrt(len(N_u)))) + Rk_iu_sum / sqrt(len(Rk_iu)) + Nk_iu_sum / sqrt(len(Nk_iu))\n",
        "\n",
        "def compute_e_ui(mat, u, i, mu, bu, bi, Rk_iu, wij, Nk_iu, cij, baseline_bu, baseline_bi, qi, pu, N_u, yj):\n",
        "    return mat[u, i] - predict_r_ui(mat, u, i, mu, bu, bi, Rk_iu, wij, Nk_iu, cij, baseline_bu, baseline_bi, qi, pu, N_u, yj)\n",
        "\n",
        "def compute_loss(mat, mu, bu, bi, Rk_iu, wij, Nk_iu, cij, baseline_bu, baseline_bi, qi, pu, N_u, yj, l_reg6=0.005, l_reg7=0.015, l_reg8=0.015):\n",
        "    loss = 0\n",
        "    cx = mat.tocoo()\n",
        "    for u,i,v in zip(cx.row, cx.col, cx.data):\n",
        "        r_ui_pred = predict_r_ui(mat, u, i, mu, bu, bi, Rk_iu, wij, Nk_iu, cij, baseline_bu, baseline_bi, qi, pu, N_u, yj)\n",
        "        Rk_iu_sum = (wij[i][Rk_iu] ** 2).sum()\n",
        "        Nk_iu_sum = (cij[i][Rk_iu] ** 2).sum()\n",
        "        loss += (mat[u, i] - r_ui_pred) ** 2 + l_reg6 * ((bu ** 2).sum() + (bi ** 2).sum()) + l_reg8 * (Rk_iu_sum + Nk_iu_sum)\n",
        "        loss += l_reg7 * ((qi[i]**2).sum() + (pu[u]**2).sum() + (yj[N_u]**2).sum())\n",
        "\n",
        "    return loss\n",
        "\n",
        "def integrated_model(mat, mat_file, gamma1=0.007, gamma2=0.007, gamma3=0.001, l_reg2=100, l_reg6=0.005, l_reg7=0.015, l_reg8=0.015, k=300, f=50):\n",
        "    # subsample the matrix to make computation faster\n",
        "    \"\"\"mat = mat[0:mat.shape[0]//128, 0:mat.shape[1]//128]\n",
        "    mat = mat[mat.getnnz(1)>0][:, mat.getnnz(0)>0]\"\"\"\n",
        "\n",
        "    print(mat.shape)\n",
        "    no_users = mat.shape[0]\n",
        "    no_movies = mat.shape[1]\n",
        "\n",
        "    #baseline_bu, baseline_bi = baseline_estimator(mat)\n",
        "    # We should call baseline_estimator but we can init at random for test\n",
        "    baseline_bu, baseline_bi = np.random.rand(no_users, 1)  * 2 - 1, np.random.rand(1, no_movies) * 2 - 1    \n",
        "\n",
        "    bu_index, bi_index = pre_processing(mat, mat_file)\n",
        "    \n",
        "    # Init parameters\n",
        "    bu = np.random.rand(no_users, 1)  * 2 - 1\n",
        "    bi = np.random.rand(1, no_movies) * 2 - 1\n",
        "    wij = np.random.rand(no_movies, no_movies) * 2 - 1\n",
        "    cij = np.random.rand(no_movies, no_movies) * 2 - 1\n",
        "    qi = np.random.rand(no_movies, f) * 2 - 1\n",
        "    pu = np.random.rand(no_users, f) * 2 - 1\n",
        "    yj = np.random.rand(no_movies, f) * 2 - 1\n",
        "\n",
        "    mu = mat.data[:].mean()\n",
        "    N = sparse.csr_matrix(mat).copy()\n",
        "    N.data[:] = 1\n",
        "    S = sparse.csr_matrix.dot(N.T, N)\n",
        "    S.data[:] = S.data[:] / (S.data[:] + l_reg2)\n",
        "    S = S * compute_sparse_correlation_matrix(mat)\n",
        "\n",
        "    # Train\n",
        "    print(\"Train...\")\n",
        "    n_iter = 200\n",
        "    cx = mat.tocoo()\n",
        "    for it in range(n_iter):\n",
        "        for u,i,v in zip(cx.row, cx.col, cx.data):\n",
        "            #Rk_iu = Nk_iu = bi_index[u]\n",
        "            N_u = bi_index[u]\n",
        "            Rk_iu = Nk_iu = np.flip(np.argsort(S[i,].toarray()))[:k].ravel()\n",
        "            e_ui = compute_e_ui(mat, u, i, mu, bu, bi, Rk_iu, wij, Nk_iu, cij, baseline_bu, baseline_bi, qi, pu, N_u, yj)\n",
        "\n",
        "            bu[u] += gamma1 * (e_ui - l_reg6 * bu[u])\n",
        "            bi[0, i] += gamma1 * (e_ui - l_reg6 * bi[0, i])\n",
        "            qi[i] += gamma2 * (e_ui * (pu[u] + 1 / sqrt(len(N_u)) * yj[N_u].sum(0)) - l_reg7 * qi[i])\n",
        "            pu[u] += gamma2 * (e_ui * qi[i] - l_reg7 * pu[u])\n",
        "            yj[N_u] += gamma2 * (e_ui * 1/ sqrt(len(N_u)) * qi[i] - l_reg7 * yj[N_u])\n",
        "            buj = mu + baseline_bu[u] + baseline_bi[0, Rk_iu]\n",
        "            wij[i][Rk_iu] += gamma3 * ( 1 / sqrt(len(Rk_iu)) * e_ui * (mat[u, Rk_iu].toarray().ravel() - buj) - l_reg8 * wij[i][Rk_iu] )\n",
        "            cij[i][Nk_iu] += gamma3 * ( 1 / sqrt(len(Nk_iu)) * e_ui - l_reg8 * cij[i][Nk_iu] )                \n",
        "        gamma1 *= 0.9\n",
        "        gamma2 *= 0.9\n",
        "        gamma3 *= 0.9\n",
        "\n",
        "        if it % 10 == 0:\n",
        "          print(it, \"\\ \", n_iter)         \n",
        "          print(\"compute loss...\")\n",
        "          print(compute_loss(mat, mu, bu, bi, Rk_iu, wij, Nk_iu, cij, baseline_bu, baseline_bi, qi, pu, N_u, yj, l_reg6=l_reg6, l_reg7=l_reg7, l_reg8=l_reg8))\n",
        "\n",
        "    return bu, bi, qi, pu, yj, wij, cij\n",
        "#################################################\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    mat_file = path+\"/T.mat\"\n",
        "    mat = io.loadmat(mat_file)['X']\n",
        "    integrated_model(mat, mat_file)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}