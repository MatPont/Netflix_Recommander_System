{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Projet Factorisation et Apprentissage.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "FVfSJT8rVKeN",
        "om6y_3YlVRBJ",
        "TUNMa40jagKf",
        "sOIvEO59VXwf",
        "-LxA5wlLhYZY",
        "AdRFoiQchpx6",
        "4GGaccjzqNGw",
        "ebKnxDPswSHw",
        "zB-4Vetz9RMz"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ypRpf9XU6hQ",
        "colab_type": "text"
      },
      "source": [
        "# Enoncé du Projet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0leEaW8UXl1-",
        "colab_type": "text"
      },
      "source": [
        "* Il s'agit d'implémenter $5$ modèles pour un système de recommendation de films à partir des données Netflix. \n",
        "\n",
        "* Les modèles à implémenter sont présentés dans l'[article](https://www.cs.rochester.edu/twiki/pub/Main/HarpSeminar/Factorization_Meets_the_Neighborhood-_a_Multifaceted_Collaborative_Filtering_Model.pdf) de Koren. Le dernier modèle de l'article est intitulé **Integrated Model**. Celui-ci a remporté le [Prix Netflix](https://en.wikipedia.org/wiki/Netflix_Prize) en $2008$. \n",
        "\n",
        "* Les données (téléchargeables à partir de ce [lien](https://archive.org/download/nf_prize_dataset.tar))  contiennent $100$ millions de notations (ratings) de $480 000$ utilisateurs sur $17 000$ films. Le but de chacun des modèles à implémenter consiste à **prédire** la notation d'un **utilisateur** donné sur l'ensemble des **films** que celui-ci n'a pas encore regardés. La recommendation pour cet utilisateur consiste à lui proposer la liste des films par ordre décroissant de notations prédites.\n",
        "\n",
        "* Chaque modèle est formulé comme une [Régression Linéaire](https://colab.research.google.com/drive/13tj_epwjFWkxBrcXwm0EELh0a81K1ID2#scrollTo=F075W1a2BWSe) dont les paramètres sont à estimer par procédure de [Gradient Descent](https://colab.research.google.com/drive/13tj_epwjFWkxBrcXwm0EELh0a81K1ID2#scrollTo=VqJhJGBduTXw). La performance de chaque modèle est évaluée par le critère RMSE (racine carrée de la Mean Squared Error vue dans le  [Chapitre Régression](https://colab.research.google.com/drive/13tj_epwjFWkxBrcXwm0EELh0a81K1ID2#scrollTo=X8iC5mkh3orV)).\n",
        "\n",
        "* Les modèles à implémenter sont:\n",
        "  1. Baseline Estimates décrit dans la section $2.1$ de l'article;\n",
        "  2. Correlation-Based Neighbourhood décrit par l'équation $(3)$ dans la section $2.2$;\n",
        "  3. Correlation-Based Neighbourhood with implicit data décrit par l'équation $(10)$ dans la section $3$;\n",
        "  4. SVD++ décrit par l'équation $(15)$ dans la section $4$;\n",
        "  5. Integrated Model décrit par l'équation $(16)$ dans la section $5$. \n",
        "\n",
        "* Dans la méthodologie ci-dessous, nous décrivons la façon d'organiser les données ainsi que les $5$ modèles à implémenter. Les notations de l'article ont été conservées afin de faciliter le basculement entre le notebook et l'article.\n",
        "\n",
        "* Le code final doit être livré en $6$ fichiers: le premier pour la compilation et des ensembles d'apprentissage et de tests; les $5$ pour chacun des modèles à implémenter. Dans le cas d'utilisation de librairies externes, mettre en commentaire la procédure d'installation de celle-ci dans chaque fichier où elle est utilisée.\n",
        "\n",
        "* Pour toute question ou incompréhension vous êtes invités à  m'écrire sur mon adresse mail: nedjmeddine.allab@gmail.com ou via  messagerie sur [linkedIn](https://www.linkedin.com/in/nedjmeddine-allab-phd-87201243/) pour des échanges plus rapides.\n",
        "\n",
        "* Bon courage.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uM8zENSoVBIk",
        "colab_type": "text"
      },
      "source": [
        "# Methodology"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GD_HZMGg5DAn",
        "colab_type": "text"
      },
      "source": [
        "## 1. Data Compilation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfWy43BX5H0v",
        "colab_type": "text"
      },
      "source": [
        "### 1.1. Generation of the rating dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2DJDaHK6L7T",
        "colab_type": "text"
      },
      "source": [
        "* Write Python function **rating_compiler** to compile from the training_set.tar, the $17770$ files and store the result into one data structure named $\\bf{D}$;\n",
        "\n",
        "* $\\bf{D}$ can be a numpy matrix, csv file/pandas DataFrame, sparse matrix, hdf5 file...;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMCp614961Sp",
        "colab_type": "text"
      },
      "source": [
        "### 1.2. Generation of training and test datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxnaEdrW64Rn",
        "colab_type": "text"
      },
      "source": [
        "* Extract from $\\bf{D}$ the ratings corresponding to the users and movies described in the probe.txt and store the result into one data structure named $\\bf{T}$;\n",
        "\n",
        "* Construct training dataset $\\bf{R}$ as $\\bf{D}$ from which we remove entries present in $\\bf{T}$;  \n",
        "\n",
        "* $\\bf{D}$, $\\bf{T}$ and $\\bf{R}$ must all have the same format (for example users as rows and films as columns);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVfSJT8rVKeN",
        "colab_type": "text"
      },
      "source": [
        "## 2. Baseline Estimates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWxQteGtVQwT",
        "colab_type": "text"
      },
      "source": [
        "* Let $u$ and $v$ be two users and $i$ and $j$ two films;\n",
        "\n",
        "* we define:\n",
        "  *  $r_{ui}$ as the rating by user $u$ on film $i$;\n",
        "  *  $\\hat{r}_{ui}$ as the **predicted** rating of $r_{ui}$;\n",
        "\n",
        "\n",
        "* In Netflix data $99\\%$ of ratings are missing, the $(u,i)$ pairs for which $r_{ui}$ is known are stored in the set\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathcal{K} = \\{(u,i) \\quad \\vert \\quad   r_{ui} \\quad \\mbox{is known}\\}.\n",
        "\\end{equation}\n",
        "\n",
        "\\\\\n",
        "\n",
        "* In rating data, we tend to have users who systematically give higher ratings than others and also, some movies which receive higher ratings than others;\n",
        "\n",
        "* In Section 2.1 of the [article](https://www.cs.rochester.edu/twiki/pub/Main/HarpSeminar/Factorization_Meets_the_Neighborhood-_a_Multifaceted_Collaborative_Filtering_Model.pdf) these tendencies are considered as baseline ratings $b_{ui}$ and are defined as\n",
        "\n",
        "\\begin{equation}\n",
        "b_{ui} = \\mu + b_u + b_i,\n",
        "\\end{equation}\n",
        "\n",
        "\\\\\n",
        "\n",
        "* where:\n",
        "  *  $\\mu$ is the overall average rating;\n",
        "  * $b_u$ the observed deviation of user $u$;\n",
        "  * $b_i$ the observed deviation of movie $i$;\n",
        "\n",
        "\\\\\n",
        "\n",
        "\n",
        "\n",
        "* Estimates of $b_u$s and $b_i$s are obtained by the minimization of the regularized MSE loss function\n",
        "\n",
        "\\begin{equation}\n",
        "\\sum_{(u,i) \\in \\mathcal{K}} (r_{ui} - b_{ui})^2 + \\lambda_1 \\left( \\sum_{u} b_u^2 + \\sum_{i} b_i^2 \\right),\n",
        "\\end{equation}\n",
        "\n",
        "\\\\\n",
        "\n",
        "  - where $\\lambda_1 \\left( \\sum_{u} b_u^2 + \\sum_{i} b_i^2 \\right)$ is the regularization term to avoid overfitting. The penality coefficient $\\lambda_1 = 0.02$.\n",
        "\n",
        "\\\\\n",
        "\n",
        "* Write a python function **baseline_estimator** to estimate $b_u, b_i$ for every $(u,i) \\in \\mathcal{K}$;\n",
        "\n",
        "\\\\\n",
        "\n",
        "* The problem above can easily be transformed into linear regression problem and the minimization of the regularized MSE can be done through gradient descent.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "om6y_3YlVRBJ",
        "colab_type": "text"
      },
      "source": [
        "## 3. Neighbourhood Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUNMa40jagKf",
        "colab_type": "text"
      },
      "source": [
        "### 3.1 Correlation-Based Neighbourhood Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f-LnkbyVTLb",
        "colab_type": "text"
      },
      "source": [
        "* To predict $r_{ui}$ neighbourhood approach look for a list of $k$ movies $\\{j_1,\\ldots,j_k \\}$ such that:\n",
        "  * the movies are already rated by $u$;\n",
        "  * the movies are appreciated by the same group of users suggesting their similarity with movie $i$;  \n",
        "\n",
        "* Similarity between movies $i$ and $j$ is defined as $s_{ij}$ and it is based on the pearson coefficient of correlation $\\rho_{ij}$   \n",
        "\n",
        "\\\\\n",
        "\n",
        "\\begin{equation}\n",
        "s_{ij} = \\frac{n_{ij}}{n_{ij} + \\lambda_2} \\rho_{ij},\n",
        "\\end{equation}\n",
        "\n",
        "\\\\\n",
        "\n",
        "* where $n_{ij}$ is the number of users who rated both movies $i$ and $j$, $\\lambda_2$ is a regulation coefficient typically equal to $100$.\n",
        "\n",
        "\\\\\n",
        "\n",
        "* To predict $r_{ui}$:\n",
        "  * define the set $S^k (i;u)$ including the $k$ movies rated by $u$ and the most similar to $i$ according to $s_{ij}$;\n",
        "  * estimate $r_{ui}$ as the weighted sum of $r_{uj}$ such that\n",
        "\n",
        "\\begin{equation}\n",
        "\\hat{r}_{ui} = b_{ui} + \\frac{1}{\\sum_{j \\in S^k (i;u)} s_{ij}} \\sum_{j \\in S^k (i;u)} s_{ij} (r_{uj} - b_{uj}).\n",
        "\\end{equation}\n",
        "\n",
        "\\\\\n",
        "\n",
        "* Write a Python function **correlation_based_neighbourhood_model** that estimate $r_{ui}$ according to the above equation. \n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hh7L9ncHa0CO",
        "colab_type": "text"
      },
      "source": [
        "### Correlation-Based Neighbourhood Model with Implicit Feedback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZeVHHE3a-4M",
        "colab_type": "text"
      },
      "source": [
        "* In Section 2.5 of [article](https://www.cs.rochester.edu/twiki/pub/Main/HarpSeminar/Factorization_Meets_the_Neighborhood-_a_Multifaceted_Collaborative_Filtering_Model.pdf), the author proposes to predict rating $r_{ui}$ from ***explicit*** neighbouring ratings $r_{uj}$ but also from ***implicit*** data such that: movie rental history or visited movies in the graphical interface;\n",
        "\n",
        "* Explicit and implicit data are formalized as follows: \n",
        "\n",
        "  * Let $u$ be a user associated with:\n",
        "    * $\\mbox{R}(u)$: set of movies for which ratings by $u$ are available;\n",
        "    * $\\mbox{N}(u)$: set of movies for which $u$ provided an implicit preference;\n",
        "\n",
        "* In the case of Netflix Data, implicit data is inferred from *which* movies have been rated. This implicitly tells us about preferences of the user;\n",
        "\n",
        "* Implicit feedback are derrived from ratings as binary matrix where:\n",
        "  * implicit preference of user $u$ to movie $i$ equals to 1 if $r_{ui}$ exists;\n",
        "  * implicit preference of user $u$ to movie $i$ equals to 0 if $r_{ui}$ does not exist;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0i4po3NlYnSH",
        "colab_type": "text"
      },
      "source": [
        "* The new model will be developped from the the following prediction rule\n",
        "\n",
        "\\\\\n",
        "\n",
        "\\begin{equation}\n",
        "\\hat{r}_{ui} = b_{ui} + \\sum_{j \\in \\mbox{R}(u)} (r_{uj} - b_{uj}) w_{ij},\n",
        "\\end{equation}\n",
        "\n",
        "* where $w_{ij}$ are associated to the similarity between movies $i$ and $j$.\n",
        "\n",
        "\\\\\n",
        "\n",
        "* Note that here the author considers $(r_{uj} - b_{uj})$ as the weight to be applied on $w_{ij}$ such that:\n",
        "\n",
        "  * $w_{ij}$ is high if movies $i$ and $j$ are related and low otherwise;\n",
        "  * in case $w_{ij}$ is high and user $u$ rated $j$ higher than expected then $(r_{uj} - b_{uj})$ will be greater and thus the contribution of $w_{ij}$ will be significant;\n",
        "  * if however $w_{ij}$ is high but user $u$ rated $j$ just as expected leading to low  $(r_{uj} - b_{uj})$ the contribution of $w_{ij}$ will not be significant;\n",
        "  * This mecanism is interesting because it gives weight only to the movies $j$ related to $i$ that user $u$ really enjoyed. \n",
        "\n",
        "* Note also that in the equation above we consider **all** movies for which user $u$ has given an **explicit** rating;\n",
        "\n",
        "* To include **implicit** feedback the author adds to the equation above another weights $c_{ij}$  \n",
        "\n",
        "\\\\\n",
        "\n",
        "\\begin{equation}\n",
        "\\hat{r}_{ui} = b_{ui} + \\sum_{j \\in \\mbox{R}(u)} (r_{uj} - b_{uj}) w_{ij} + \\sum_{j \\in \\mbox{N}(u)} c_{ij},\n",
        "\\end{equation}\n",
        "\n",
        "\\\\\n",
        "\n",
        "* where $c_{ij}$ is high if movie $j$ is related to $i$ and if user $u$ implicitly said that $j$ belongs to his category of films.\n",
        "\n",
        "\\\\\n",
        "\n",
        "* The author suggests to normalize the equation above by the suqared root of $\\mbox{R}(u)$ and $\\mbox{N}(u)$ sizes   \n",
        "\n",
        "\\\\\n",
        "\n",
        "\\begin{equation}\n",
        "\\hat{r}_{ui} = b_{ui} + \\frac{1}{\\sqrt{|\\mbox{R}(u)|}}\\sum_{j \\in \\mbox{R}(u)} (r_{uj} - b_{uj}) w_{ij} + \\frac{1}{\\sqrt{|\\mbox{N}(u)|}}\\sum_{j \\in \\mbox{N}(u)} c_{ij},\n",
        "\\end{equation}\n",
        "\n",
        "\\\\\n",
        "\n",
        "* For a better computation efficiency, only the $k$ nearest movies will be selected from explicit and implicit sets and respectively stored in $\\mbox{R}^k (i;u)$ and $\\mbox{N}^k (i;u)$.\n",
        "\n",
        "\\\\\n",
        "\n",
        "* Finally, the prediction rule is defined as follows\n",
        "\n",
        "\\\\\n",
        "\n",
        "\\begin{equation}\n",
        "\\hat{r}_{ui} = \\mu + b_u + b_i + \\frac{1}{\\sqrt(|\\mbox{R}^k (i;u)|)}\\sum_{j \\in \\mbox{R}^k (i;u)} (r_{uj} - b_{uj}) w_{ij} + \\frac{1}{\\sqrt(|\\mbox{N}^k (i;u)|)}\\sum_{j \\in \\mbox{N}^k (i;u)} c_{ij}.\n",
        "\\end{equation}\n",
        "\n",
        "\\\\\n",
        "\n",
        "* The $b_{uj}$ of the $4^{th}$ term are the baseline prediction estimated in your first function. In this new model they are considered as constants;\n",
        "\n",
        "* The parameters are: $b_u, b_i, w_{ij}, c_{ij}$ estimated by the minimization of the regularized MSE loss function\n",
        "\n",
        "\\\\\n",
        "\n",
        "\\begin{equation}\n",
        "\\sum_{(u,i) \\in \\mathcal{K}} (r_{ui} - \\hat{r}_{ui})^2 + \\lambda_4 \\left( \\sum_{u} b_u^2 + \\sum_{i} b_i^2  + \\sum_{j \\in \\mbox{R}^k (i;u)} w_{i,j}^2 + \\sum_{j \\in \\mbox{N}^k (i;u)} c_{i,j}^2 \\right).\n",
        "\\end{equation}\n",
        "\n",
        "\\\\\n",
        "\n",
        "* The author suggests regularization coefficient $\\lambda_4 = 0.002$.\n",
        "\n",
        "* Write a Python function **correlation_based_implicit_neighbourhood_model** that estimate $r_{ui}$ according to the above equation. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOIvEO59VXwf",
        "colab_type": "text"
      },
      "source": [
        "## 4. Latent Factor Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LxA5wlLhYZY",
        "colab_type": "text"
      },
      "source": [
        "#### Singular Value Decomposition\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcuOCCboVaXA",
        "colab_type": "text"
      },
      "source": [
        "* Latent factor models explain observed ratings as a dot product between two latent (or feature) vectors $p_u$ and $q_i$ both defined in  ${\\rm I\\!R}^f$;\n",
        "\n",
        "* In the context of recommender systems, $p_u$ may describe user $u$ in terms of his belonging to $f$ user categories while $q_i$, may represent for the movie $i$ the proportions of $f$ genres that it contains;\n",
        "\n",
        "* Singular Value Decomposition ([SVD](https://colab.research.google.com/drive/1xJHWm1LS39u4NDxnRsfjaxrux8HE1nbg#scrollTo=rgGYLnWH42aC)) of user-movie rating matrix is widely used and lead to the following prediction model  \n",
        "\n",
        "\\\\\n",
        "\n",
        "\\begin{equation}\n",
        "\\hat{r}_{ui} = b_{ui} + p_u^T q_i.\n",
        "\\end{equation}\n",
        "\n",
        "\\\\\n",
        "\n",
        "* In practice, SVD raises difficulties due to the high portion of missing ratings ($99\\%$ in the Netflix Dataset), we tend to model only the observed ratings by minimizing the regularized MSE loss function\n",
        "\n",
        "\\\\\n",
        "\n",
        "\\begin{equation}\n",
        "\\sum_{(u,i) \\in \\mathcal{K}} (r_{ui} - \\hat{r}_{ui})^2 + \\lambda_3 \\left( \\sum_{u} b_u^2 + \\sum_{i} b_i^2  + ||p_u||^2 + ||q_i||^2 \\right).\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdRFoiQchpx6",
        "colab_type": "text"
      },
      "source": [
        "### Improved Singular Value Decomposition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DM2mhkKthtmM",
        "colab_type": "text"
      },
      "source": [
        "* **Patereck** proposes in his [article](https://www.cs.uic.edu/~liub/KDD-cup-2007/proceedings/Regular-Paterek.pdf) to model $p_u$ only on rated movies;\n",
        "\n",
        "* In that case, each movie $i$ becomes associated with **two** factors vectors $q_i$ and $x_i$ such that each user $u$ will be represented by the factor vector \n",
        "\n",
        "\\\\\n",
        "\n",
        "\\begin{equation}\n",
        "p_u = \\frac{1}{\\sqrt{ | \\mbox{R}(u) |}} \\sum_{j \\in \\mbox{R}(u)} x_j,\n",
        "\\end{equation}\n",
        "\n",
        "\\\\\n",
        "\n",
        "* leading to the new prediction model\n",
        "\n",
        "\\\\\n",
        "\n",
        "\\begin{equation}\n",
        "\\hat{r}_{ui} = b_{ui} + q_i^T \\frac{1}{\\sqrt{ | \\mbox{R}(u) |}} \\sum_{j \\in \\mbox{R}(u)} x_j.\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GGaccjzqNGw",
        "colab_type": "text"
      },
      "source": [
        "### Asymmetric SVD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNleAh2cqNYb",
        "colab_type": "text"
      },
      "source": [
        "* Following the improved SVD paradigm, **Koren** came up with asymmetric SVD that includes implicit feedback;\n",
        "\n",
        "* Each movie $i$ is associated with $3$ factors vectors $q_i, x_i, y_i \\in {\\rm I\\!R}^f$ leading to the following prediction model\n",
        "\n",
        "\\\\\n",
        "\n",
        "\\begin{equation}\n",
        "\\hat{r}_{ui} = b_{ui} + q_i^T \\left( \\frac{1}{\\sqrt{ | \\mbox{R}(u) |}} \\sum_{j \\in \\mbox{R}(u)} (r_{uj} - b_{uj}) x_j +  \\frac{1}{\\sqrt{ | \\mbox{N}(u) |}}\\sum_{j \\in \\mbox{N}(u)} y_j \\right).\n",
        "\\end{equation}\n",
        "\n",
        "\\\\\n",
        "\n",
        "* This model presents many advantages such as:\n",
        "  * new users do not require retraining as it depends only on movies;\n",
        "  * recommendations can be explained in terms of movies only;\n",
        "  * it has less parameters when the number of users is larger than the number of films;\n",
        "\n",
        "\\\\\n",
        "\n",
        "* Just like the other models, parameters are estimated through the minimization of regularized MSE loss function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebKnxDPswSHw",
        "colab_type": "text"
      },
      "source": [
        "### SVD++"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Evm5cPdGwUqA",
        "colab_type": "text"
      },
      "source": [
        "* **Koren** highlighted in Section 4 of his [article](https://www.cs.rochester.edu/twiki/pub/Main/HarpSeminar/Factorization_Meets_the_Neighborhood-_a_Multifaceted_Collaborative_Filtering_Model.pdf) that introducing implicit feedback directly in the SVD prediction rule will yield better results\n",
        "\n",
        "\\\\\n",
        "\n",
        "\\begin{equation}\n",
        "\\hat{r}_{ui} = b_{ui} + q_i^T \\left( p_u  +  \\frac{1}{\\sqrt{ | \\mbox{N}(u) |}}\\sum_{j \\in \\mbox{N}(u)} y_j \\right).\n",
        "\\end{equation}\n",
        "\n",
        "\\\\\n",
        "\n",
        "* Write a Python program named **SVD++** that estimate $r_{ui}$ according to the above equation. Estimation of parameters is done through the minimization of a regularized MSE Loss function.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zB-4Vetz9RMz",
        "colab_type": "text"
      },
      "source": [
        "## 5. Integrated Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPfId6tc9RYh",
        "colab_type": "text"
      },
      "source": [
        "* Empiric research shows that Neighborhood\n",
        "and factor models do not capture same informations; \n",
        "\n",
        "* **Koren** ultimately proposed in Section 5 of his [article](https://www.cs.rochester.edu/twiki/pub/Main/HarpSeminar/Factorization_Meets_the_Neighborhood-_a_Multifaceted_Collaborative_Filtering_Model.pdf) to combine between Latent and Neighbourhood models to enrich each other as \n",
        "\n",
        "\\\\\n",
        "\n",
        "\\begin{equation}\n",
        "\\hat{r}_{ui} = \\mu + b_u + b_i + q_i^T \\left( p_u  +  \\frac{1}{\\sqrt{ | \\mbox{N}(u) |}}\\sum_{j \\in \\mbox{N}(u)} y_j \\right) + \\frac{1}{\\sqrt{|\\mbox{R}(u)|}}\\sum_{j \\in \\mbox{R}(u)} (r_{uj} - b_{uj}) w_{ij} + \\frac{1}{\\sqrt{|\\mbox{N}(u)|}}\\sum_{j \\in \\mbox{N}(u)} c_{ij}.\n",
        "\\end{equation}\n",
        "\n",
        "\\\\\n",
        "\n",
        "* First three terms describe general properties of the movie and the user;\n",
        "\n",
        "* Forth term provides the interaction between user's profile and  movie's profile;\n",
        "\n",
        "* Last two terms contributes fine grained adjustments that are hard to profile;\n",
        "\n",
        "* Write Python program named **integrated_model** that estimate $r_{ui}$ according to the above equation by minimizing regularized MSE Loss function."
      ]
    }
  ]
}